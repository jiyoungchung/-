## data_project

#### - 성능 지표 : MAE (실제 값과 예측 값의 차이(Error)를 절대값으로 변환해 평균화), 정확도(accuracy_score)

#### - model 선정 기준 : 위에 성능지표를 기준으로 MAE가 작은 것, 정확도가 높은 모델을 선정함

#### - 사용 모델 : xgboost, random forest, LightGBM

#### - 모델 비교
##### (1) xgboost 
- 중식계 : accuracy_score(0.725), MAE(82.9)
<p align="left"><img src="https://user-images.githubusercontent.com/88172522/160308656-fa0b3f24-1b44-4c89-bb08-1ba185ebe5d0.png"></p>

- 석식계 : accuracy_score(0.717), MAE(50.8)
<p align="left"><img src="https://user-images.githubusercontent.com/88172522/160308671-6cdefa1a-3022-47a8-ae48-43509266ff6d.png"></p>

##### (2) random forest
- 중식계 : accuracy_score(0.0), MAE(105.85)

- 석식계 : accuracy_score(0.03), MAE(67.726)

##### (3) LightGBM
- 중식계 : accuracy_score(0.0), MAE(149.402)

- 석식계 : accuracy_score(0.03), MAE(83.224)

#### 모델 설명
##### (1) xgboost?
- "현존 머신러닝 모델중 가장 우월한 XGBoost 모델"
- XGBoost의 장점 : 빠른 수행시간 / 분류와 회귀영역에서 뛰어난 예측 성능 발휘 등
- 부스팅이란? 여러 개의 약한 Decision Tree를 조합해서 사용하는 Ensemble 기법 중 하나 (즉, 약한 예측 모형들의 학습 에러에 가중치를 두고, 순차적으로 다음 학습 모델에 반영하여
강한 예측모형을 만드는 것)

##### (2) Random Forest
- "무작위 숲" ; 훈련을 통해 구성해 놓은 다수의 나무들로부터 분류결과를 취합해서 결론을 얻는 모델
- 원리? 중복허용(부트스트랩) 기법을 이용해서 다수의 train data 생성 -> 생성된 train data로 결정나무 모델 구축(배깅 방식으로) -> 의사결정나무 모델 구축 시 변수를 무작위로 선택 -> 예측

##### (3) LightGBM
- "XGBoost와 함께 부스팅 계열 알고리즘에 각광받는 모델"
- 장점 : 빠른 학습 및 예측 수행 시간 / 더 작은 메모리 사용량 / 카테코리형 피처의 자동 변환 및 최적 분할
- Gradient Boosting 프레임워크로 Tree기반 학습 알고리즘 / 기존의 다른 Tree기반 알고리즘과 다른점은 Tree구조가 수평적으로 확장하는 다른 Tree기반 알고리즘에 비해 수직적으로 확장을 하는것에 있음


#### 결과
- 정확도와 MAE기준으로 선정한 모델은 "xgboost"이다.
- 혼자 진핸하는 프로젝트는 처음이라서 보완할 점이 많을 것이라고 생각한다.
- 그래도 혼자 가설을 생각해보고 검증 및 시각화 해보는 과정에서 데이터를 통해 인사이트를 도출하는 부분이 흥미가 있었다.



